<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Language Models, Ztrura">
    <meta name="description" content="大模型学习笔记之语言模型综述。">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Language Models | Ztrura</title>
    <link rel="icon" type="image/png" href="/dog.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    
        <link rel="stylesheet" type="text/css" href="/css/reward.css">
    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 6.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/dog.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Ztrura</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/dog.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Ztrura</div>
        <div class="logo-desc">
            
            Be here now.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Ztrura" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Ztrura" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Language Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/NLP/">
                                <span class="chip bg-color">NLP</span>
                            </a>
                        
                            <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">
                                <span class="chip bg-color">大模型学习笔记</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/IDEAS-Lab/" class="post-category">
                                IDEAS Lab
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2023-01-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2023-01-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.8k
                </div>
                

                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.min.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h2><h4 id="Epoch"><a href="#Epoch" class="headerlink" title="Epoch"></a>Epoch</h4><p>当一个完整的数据集通过了神经网络一次并且返回了一次，这个过程称为一次epoch。（也就是说，所有训练样本在神经网络中都进行了一次正向传播和一次反向传播 ）</p>
<p>再通俗一点，一个Epoch就是将所有训练样本训练一次的过程。</p>
<p>然而，当一个Epoch的样本（也就是所有的训练样本）数量可能太过庞大（对于计算机而言），就需要把它分成多个小块，也就是就是分成多个Batch 来进行训练。</p>
<h4 id="Batch"><a href="#Batch" class="headerlink" title="Batch"></a>Batch</h4><p>批 / 一批样本。</p>
<h4 id="Batch-Size"><a href="#Batch-Size" class="headerlink" title="Batch_Size"></a>Batch_Size</h4><p>每批样本的大小。</p>
<h4 id="Iteration"><a href="#Iteration" class="headerlink" title="Iteration"></a>Iteration</h4><p>一次迭代。训练一个Batch就是一次Iteration（这个概念跟程序语言中的迭代器相似）</p>
<h4 id="Perplexity"><a href="#Perplexity" class="headerlink" title="Perplexity"></a>Perplexity</h4><p>PPL是用在自然语言处理领域（NLP）中，衡量语言模型好坏的指标。它主要是根据每个词来估计一句话出现的概率，并用句子长度作normalize。PPL越小，一句我们期望的sentence出现的概率就越高。</p>
<h4 id="Zero-shot-learning"><a href="#Zero-shot-learning" class="headerlink" title="Zero-shot learning"></a>Zero-shot learning</h4><p>Zero-shot learning：零样本学习。</p>
<p>利用高维语义特征代替样本的低维特征，使得训练出来的模型具有迁移性。</p>
<p>舍去低维特征，不需要“面面俱到”，达到分类目的。</p>
<h4 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h4><p>主要应用：图片处理。</p>
<p>背景：</p>
<ol>
<li>图像需要处理的数据量太大，导致成本很高，效率很低</li>
<li>图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高</li>
</ol>
<p>结构：</p>
<ul>
<li><p>卷积层：使用卷积核来过滤图像的各个小区域。</p>
<p><img src="image-20230110183519954.png" alt="Convolved Feature"></p>
<p>在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。</p>
<p>总结：通过卷积核的过滤提取出图片中局部的特征。</p>
</li>
<li><p>池化层：数据降维，避免过拟合。</p>
<p><img src="image-20230110183642038.png" alt="Pooled Feature"></p>
</li>
<li><p>全连接层：输出结果。</p>
<p>在全连接层之前可以有多个卷积层和池化层。</p>
<p><img src="image-20230108171350085.png" alt="Full Connection"></p>
</li>
</ul>
<h4 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h4><p>CNN 和普通的算法大部分都是输入和输出的一一对应，也就是一个输入得到一个输出。不同的输入之间是没有联系的。</p>
<p>RNN 跟传统神经网络最大的区别在于每次都会将前一次的输出结果，带到下一次的隐藏层中，一起训练。</p>
<p><img src="image-20230110183721847.png" alt="RNN"></p>
<p>问题：短期记忆影响较大，无法处理很长的输入序列；由于依赖前步输出，无法进行并行计算。</p>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p>LSTM 可以保留较长序列数据中的“重要信息”，忽略不重要的信息。这样就解决了 RNN 短期记忆的问题。</p>
<h4 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h4><p>Gated Recurrent Unit – GRU 是 LSTM 的一个变体，他主要是在 LSTM 的模型上做了一些简化和调整，在训练数据集比较大的情况下可以节省很多时间。</p>
<h4 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h4><p>Softmax 是一种激活函数，它可以将一个数值向量归一化为一个概率分布向量，且各个概率之和为1。Softmax 可以用来作为神经网络的最后一层，用于多分类问题的输出。</p>
<h2 id="Encoder-Decoder-amp-Seq2Seq"><a href="#Encoder-Decoder-amp-Seq2Seq" class="headerlink" title="Encoder-Decoder &amp; Seq2Seq"></a>Encoder-Decoder &amp; Seq2Seq</h2><h4 id="Encoder-Decoder-模型"><a href="#Encoder-Decoder-模型" class="headerlink" title="Encoder-Decoder 模型"></a>Encoder-Decoder 模型</h4><p>不是某种具体的算法，而是一类算法的统称。</p>
<p>机器学习的核心思路：将现实问题转化为数学问题，通过求解数学问题，从而解决现实问题。</p>
<p><img src="image-20230108110644217.png" alt="Encoder"></p>
<p><img src="image-20230108110653654.png" alt="Decoder"></p>
<p><img src="image-20230108110713934.png" alt="Encoder-Decoder"></p>
<p>需要说明的点：</p>
<ol>
<li>无论输入输出长度如何，中间的向量C长度固定（这也是缺陷）。</li>
<li>根据不同的问题选择不同的 Encoder 和 Decoder，可以是一个 RNN，但通常是其变种 LSTM 或者 GRU。</li>
</ol>
<p>应用：</p>
<ul>
<li>text - text</li>
<li>audio - text</li>
<li>image/video - text</li>
</ul>
<h4 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h4><p>字面意思，输入一个序列，输出一个序列。</p>
<p>特点：输入序列和输出序列的长度可变。</p>
<p><em>在 Seq2Seq 框架提出之前，深度神经网络在图像分类等问题上取得了非常好的效果。在其擅长解决的问题中，输入和输出通常都可以表示为固定长度的向量，如果长度稍有变化，会使用补零等操作。然而许多重要的问题，例如机器翻译、语音识别、自动对话等，表示成序列后，其长度事先并不知道。因此如何突破先前深度神经网络的局限，使其可以适应这些场景，成为了13年以来的研究热点，Seq2Seq 框架应运而生。</em></p>
<p>属于 Encoder-Decoder 的大范畴。</p>
<p>缺点：非常占内存；大数据量上不容易调参。</p>
<h4 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h4><h5 id="Encoder-Decoder-模型的缺陷："><a href="#Encoder-Decoder-模型的缺陷：" class="headerlink" title="Encoder-Decoder 模型的缺陷："></a>Encoder-Decoder 模型的缺陷：</h5><p>Encoder 和 Decoder 之间只有一个向量 C 来传递信息，且 C 的长度固定 → 输入信息过长时，会丢失掉一部分信息。</p>
<p>Attention 机制就是为了解决”信息过长，信息丢失”的问题。</p>
<h5 id="引入了-Attention-机制的-Encoder-Decoder-模型"><a href="#引入了-Attention-机制的-Encoder-Decoder-模型" class="headerlink" title="引入了 Attention 机制的 Encoder-Decoder 模型"></a>引入了 Attention 机制的 Encoder-Decoder 模型</h5><p>Encoder 不再将整个输入序列编码为固定长度的中间向量 C，而是编码为一个向量的序列。</p>
<p><img src="image-20230108121728990.png" alt="Encoder-Decoder with attention"></p>
<p>在产生每一个输出的时候，都能够做到充分利用输入序列所携带的信息。</p>
<p>那么，什么是 Attention 机制？</p>
<h2 id="Attention-机制"><a href="#Attention-机制" class="headerlink" title="Attention 机制"></a>Attention 机制</h2><h4 id="本质"><a href="#本质" class="headerlink" title="本质"></a>本质</h4><p>关注全部 → 关注重点。</p>
<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><h5 id="参数少"><a href="#参数少" class="headerlink" title="参数少"></a>参数少</h5><p>模型复杂度相对 CNN、RNN 等更小，参数也更少，对算力要求较低。</p>
<h5 id="速度快"><a href="#速度快" class="headerlink" title="速度快"></a>速度快</h5><p>解决了 RNN 不能并行计算的问题，Attention 机制的每一步计算都不依赖上一步的计算结果，因此可以和 CNN 一样并行处理。</p>
<h5 id="效果好"><a href="#效果好" class="headerlink" title="效果好"></a>效果好</h5><p>Attention 机制引入之前：就像是遥远的记忆会变得模糊一样，长距离的信息会被弱化。</p>
<p>Attention 机制引入之后：挑重点进行处理，文本较长不妨碍抓住重点。</p>
<h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>简单理解：encoder 层的输出经过加权平均后再输入到 decoder 层中。</p>
<p><img src="image-20230108124359908.png" alt="Attention"></p>
<ol>
<li>对 query 和 key 进行相似度计算，得到权值；</li>
<li>将权值进行归一化，得到直接可用的权重；</li>
<li>将权重和 value 进行加权求和。</li>
</ol>
<h4 id="类型"><a href="#类型" class="headerlink" title="类型"></a>类型</h4><p><img src="image-20230108124824075.png" alt="Attention种类"></p>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><h4 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h4><p>左边被”N×“的为 Encoder，右边被“N×”的为 Decoder。</p>
<p><img src="image-20230108182646123.png" alt="Transformer"></p>
<h4 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h4><p>表达元素内部之间的 attention 关系，输入之间的 QKV 会互相影响。</p>
<h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><p>”多头“：对同一key、value、query抽取不同信息，相当于平行复制后随机初始化参数，从而使每个 self-attention 模块具有不同的假设空间。</p>
<p>使用多个独立的 Attention 池化，合并每个 head 的输出得到最终输出。</p>
<p>目的：这种结构设计能让每个注意力机制通过 QKV 映射到不同的空间去学习特征，去优化每个词汇的不同特征部分，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元的表达，实验表明可以从而提升模型效果。</p>
<p><img src="image-20230108174515578.png" alt="Multi-Head Attention"></p>
<h4 id="Masked-Multi-Head-Attention"><a href="#Masked-Multi-Head-Attention" class="headerlink" title="Masked Multi-Head Attention"></a>Masked Multi-Head Attention</h4><p>Decoder 对序列中某个元素进行输出时，不应该考虑该元素之后的元素。</p>
<p>↓ 通过掩码实现</p>
<p>计算 $x_i$ 输出时，假装当前序列长度为 $i$。</p>
<h4 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h4><p>除了注意力子层，编码器和解码器中的每一层都包含一个全连接的前馈网络，该网络分别且相同地应用于每个位置。这包括两个线性转换，中间有一个 ReLu 激活。</p>
<p><img src="image-20230108183006265.png" alt="Feed-Forward"></p>
<p>作用：将输入形状由 (b, n, d) 变换为 (bn, d)、输出形状由 (bn, d) 变换为 (b, n, d)。避免了n 的变化导致的输入维度不稳定。</p>
<p>b: batchsize，多少个句子；</p>
<p>n: 序列长度，句子中有多少个字；</p>
<p>d: dimension，字是多少维。</p>
<h4 id="Add-amp-Norm"><a href="#Add-amp-Norm" class="headerlink" title="Add &amp; Norm"></a>Add &amp; Norm</h4><p>Add：</p>
<p>将 Multi-Head Attention 的输入与输出加起来。</p>
<p>Norm： </p>
<p>Layer Normalization。</p>
<p>不能用 Batch Normalization：</p>
<p>BN 是每个通道样本间归一化，LN 是每个样本通道间归一化。</p>
<p>有 b 句话，每句话有 len 个词，每个词由 d 个特征表示，BN 是对所有句子所有词的某一特征做归一化，LN 是对某一句话的所有词所有特征做归一化。</p>
<p>不同句子字数不同，无法进行句间归一化。</p>
<h4 id="信息传递"><a href="#信息传递" class="headerlink" title="信息传递"></a>信息传递</h4><p>将 Encoder 的输出 y1 ~ yn 分别给到 Decoder 中的每个块。</p>
<h4 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h4><p>Transformer 中的输入由 Word Embedding 和 Positional Embedding 相加得到。</p>
<h5 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h5><p>使用 Word2Vec、Glove等算法预训练得到，也可以在 Transformer 中训练得到。</p>
<h5 id="Positional-Embedding"><a href="#Positional-Embedding" class="headerlink" title="Positional Embedding"></a>Positional Embedding</h5><p>表示单词出现在句子中的位置。</p>
<p><img src="image-20230108200234061.png" alt="Positional Embedding"></p>
<p>pos 表示单词在句中的位置，d 表示 PE 的维度（与 Word Embedding 一致），2i 表示偶数维度，2i+1 表示奇数维度。</p>
<p>为什么这样表示？</p>
<ol>
<li>使 PE 能够适应比训练集里面所有句子更长的句子，假设训练集里面最长的句子是有 20 个单词，突然来了一个长度为 21 的句子，则使用公式计算的方法可以计算出第 21 位的 Embedding。</li>
<li>可以让模型容易地计算出相对位置，对于固定长度的间距 k，PE(pos+k) 可以用 PE(pos) 计算得到。因为 Sin(A+B) = Sin(A)Cos(B) + Cos(A)Sin(B), Cos(A+B) = Cos(A)Cos(B) - Sin(A)Sin(B)。</li>
</ol>
<h2 id="Pre-train-Model"><a href="#Pre-train-Model" class="headerlink" title="Pre-train Model"></a>Pre-train Model</h2><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p>标注资源稀缺而无标注资源丰富：某种特殊的任务只存在非常少量的相关训练数据，以至于模型不能从中学习总结到有用的规律。</p>
<h4 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h4><ol>
<li>模型角度：模型参数不再是随机初始化，而是通过某些特定任务（如语言模型）进行预训练；</li>
<li>数据角度：将训练任务拆解成共性学习和特性学习两个步骤。</li>
</ol>
<h4 id="学习任务的分解"><a href="#学习任务的分解" class="headerlink" title="学习任务的分解"></a>学习任务的分解</h4><p>“预训练“的做法一般是将大量低成本收集的训练数据放在一起，经过某种预训方法去学习其中的共性，然后将其中的共性“移植”到特定任务的模型中，再使用相关特定领域的少量标注数据进行“微调”，这样的话，模型只需要从”共性“出发，去“学习”该特定任务的“特殊”部分即可。</p>
<p>举例理解：会说汉语→法律文献专业名词提取 vs 不会说汉语→法律文献专业名词提取。</p>
<p>先学习汉语，再进行法律文献专业名词提取→学习任务分解。</p>
<h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><p>自编码预训练（AE）：BERT，对应了 encoder 的预训练，更适合文本理解任务；</p>
<p>自回归预训练（AR）：GPT，对应了 decoder 的预训练，更适合文本生成任务。</p>
<h2 id="BERT-amp-RoBERTa"><a href="#BERT-amp-RoBERTa" class="headerlink" title="BERT &amp; RoBERTa"></a>BERT &amp; RoBERTa</h2><h4 id="BERT-Bidirectional-Encoder-Representation-from-Transformers"><a href="#BERT-Bidirectional-Encoder-Representation-from-Transformers" class="headerlink" title="BERT (Bidirectional Encoder Representation from Transformers)"></a>BERT (Bidirectional Encoder Representation from Transformers)</h4><h5 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h5><p>BERT 不再像以往一样采用传统的单向语言模型或者把两个单向语言模型进行浅层拼接的方法进行预训练，而是采用新的 masked language model（MLM），以致能生成深度的双向语言表征。</p>
<h5 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h5><ol>
<li>采用 MLM 对双向的 Transformer 进行预训练，以生成能融合左右上下文信息的深层双向语言表征。</li>
<li>预训练后，只需要添加一个额外的输出层进行微调，就可以在各种各样的下游任务中取得 state-of-the-art 的表现。在这过程中并不需要对 BERT 进行任务特定的结构修改。</li>
</ol>
<h5 id="预训练过程"><a href="#预训练过程" class="headerlink" title="预训练过程"></a>预训练过程</h5><h6 id="MLM"><a href="#MLM" class="headerlink" title="MLM"></a>MLM</h6><p>MLM可以理解为完形填空，作者会随机mask每一个句子中15%的词，用其上下文来做预测，例如：my dog is hairy → my dog is [MASK]。</p>
<p>此处将hairy进行了mask处理，然后采用非监督学习的方法预测mask位置的词是什么，但是该方法有一个问题，因为是 mask 15%的词，其数量已经很高了，这样就会导致某些词在fine-tuning阶段从未见过，为了解决这个问题，作者做了如下的处理：</p>
<ul>
<li>80%的时间是采用[mask]，my dog is hairy → my dog is [MASK]；</li>
<li>10%的时间是随机取一个词来代替mask的词，my dog is hairy -&gt; my dog is apple；</li>
<li>10%的时间保持不变，my dog is hairy -&gt; my dog is hairy；</li>
</ul>
<p>使用随机词的原因：防止Transformer直接将[MASK]记为”hairy”，同时迫使模型更多地依赖于上下文信息来预测词汇。</p>
<h6 id="Next-Sentence-Prediction"><a href="#Next-Sentence-Prediction" class="headerlink" title="Next Sentence Prediction"></a>Next Sentence Prediction</h6><p>对 Sentence A 与其之后的 Sentence B 进行训练，其中 B 中50%的数据为 A 的下一条句子，剩余50%的数据为语料库中的随机句子。</p>
<p>使预训练的模型可以处理需要理解两个句子之间的关系的任务。</p>
<h5 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h5><p>BERT只使用了Transformer的Encoder模块，原论文中，作者分别用12层和24层Transformer Encoder组装了两套BERT模型，分别是：</p>
<p><img src="image-20230109201517021.png" alt="BERT"></p>
<p>其中层的数量（即 Transformer Encoder 块的数量）为 L，隐藏层的维度为 H，自注意头的个数为 A。在所有例子中，我们将前馈/过滤器（ Transformer Encoder 端的 feed-forward 层）的维度设置为 4H，即当 H=768 时是 3072；当 H=1024 是 4096。</p>
<p><img src="image-20230109201814452.png" alt="BERT模型"></p>
<h5 id="输入-1"><a href="#输入-1" class="headerlink" title="输入"></a>输入</h5><p>BERT的输入词向量是三个向量之和：</p>
<ul>
<li>Token Embedding：词向量。</li>
<li>Segment Embedding：表明这个词属于哪个句子（NSP 需要两个句子）。</li>
<li>Position Embedding：学习出来的 embedding 向量。这与 Transformer 不同，Transformer 中是预先设定好的值。</li>
</ul>
<p><img src="image-20230109204017869.png" alt="Input of BERT"></p>
<h5 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h5><p>BERT预训练模型的输出结果，无非就是一个或多个向量。下游任务可以通过精调（改变预训练模型参数）或者特征抽取（不改变预训练模型参数，只是把预训练模型的输出作为特征输入到下游任务）两种方式进行使用。</p>
<h5 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h5><ul>
<li>引入了Masked LM，使用双向 LM 做模型预训练；</li>
<li>为预训练引入了新目标 NSP，它可以学习句子与句子间的关系；</li>
<li>进一步验证了更大的模型效果更好： 12 —&gt; 24 层；</li>
<li>为下游任务引入了很通用的求解框架，不再为任务做模型定制；</li>
<li>刷新了多项 NLP 任务的记录，引爆了NLP无监督预训练技术。</li>
</ul>
<h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><ul>
<li>[MASK]标记在实际预测中不会出现，训练时用过多[MASK]影响模型表现；</li>
<li><p>每个batch只有15%的token被预测，所以BERT收敛得比 left-to-right 模型要慢（它们会预测每个token）；</p>
</li>
<li><p>BERT对硬件资源的消耗巨大，大模型需要16个 tpu，历时四天；更大的模型需要64个 tpu，历时四天。</p>
</li>
</ul>
<h5 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h5><ul>
<li>句子或者段落的匹配类任务：Bert在预训练阶段增加了 NSP 任务，所以能够在预训练阶段学会一些句间关系的知识，而如果下游任务正好涉及到句间关系判断，就特别吻合 Bert 本身的长处。</li>
<li>适合解决输入长度不太长的NLP任务：而输入比较长的任务，典型的比如文档级别的任务，Bert解决起来可能就不太好。主要原因在于：Transformer的self attention机制因为要对任意两个单词做attention计算，所以时间复杂度是n平方，n是输入的长度。如果输入长度比较长，Transformer的训练和推理速度掉得比较厉害，于是，这点约束了Bert的输入长度不能太长。所以对于输入长一些的文档级别的任务，Bert就不容易解决好。结论是：Bert更适合解决句子级别或者段落级别的NLP任务。</li>
</ul>
<h4 id="RoBERTa（-A-Robustly-Optimized-BERT-Pretraining-Approach）"><a href="#RoBERTa（-A-Robustly-Optimized-BERT-Pretraining-Approach）" class="headerlink" title="RoBERTa（ A Robustly Optimized BERT Pretraining Approach）"></a>RoBERTa（ A Robustly Optimized BERT Pretraining Approach）</h4><p>从模型上来说，RoBERTa基本没有什么太大创新，主要是改变了预训练的方法。</p>
<h5 id="静态-Masking-vs-动态-Masking"><a href="#静态-Masking-vs-动态-Masking" class="headerlink" title="静态 Masking vs 动态 Masking"></a>静态 Masking vs 动态 Masking</h5><p>Bert对每一个序列随机选择15%的Tokens替换成[MASK]，但整个训练过程，这15%的Tokens一旦被选择就不再改变。</p>
<p>RoBERTa一开始把预训练的数据复制10份，每一份都随机选择15%的Tokens进行Masking，也就是说，同样的一句话有10种不同的mask方式。然后每份数据都训练N/10个epoch。这就相当于在这N个epoch的训练中，每个序列的被mask的tokens是会变化的。</p>
<p><img src="image-20230109234012371.png" alt="静态 Masking vs 动态 Masking"></p>
<h5 id="with-NSP-vs-without-NSP"><a href="#with-NSP-vs-without-NSP" class="headerlink" title="with NSP vs without NSP"></a>with NSP vs without NSP</h5><p>RoBERTa去除了NSP，而是每次输入连续的多个句子，直到最大长度512（可以跨文章）。这种训练方式叫做（FULL - SENTENCES），而原来的Bert每次只输入两个句子。实验表明在MNLI这种推断句子关系的任务上RoBERTa也能有更好性能。</p>
<p><img src="image-20230109234442993.png" alt="with NSP vs without NSP"></p>
<h5 id="更大的mini-batch"><a href="#更大的mini-batch" class="headerlink" title="更大的mini-batch"></a>更大的mini-batch</h5><p>原本的 BERTbase 的batch size是256，训练 1M 个steps。RoBERTa 的 batch size 为 8k。作者借鉴了在机器翻译中，用更大的batch size配合更大学习率能提升模型优化速率和模型性能的现象，并且也用实验证明了确实Bert还能用更大的batch size。</p>
<p><img src="image-20230109235207656.png" alt="更大的mini-batch"></p>
<h5 id="更多的数据，更长时间的训练"><a href="#更多的数据，更长时间的训练" class="headerlink" title="更多的数据，更长时间的训练"></a>更多的数据，更长时间的训练</h5><p>借鉴 XLNet 用了比 Bert 多10倍的数据，RoBERTa也用了更多的数据，配合更长时间的训练，性能再次提高。</p>
<p><img src="image-20230109235356087.png" alt="更多的数据，更长时间的训练"></p>
<h2 id="GPT-系列"><a href="#GPT-系列" class="headerlink" title="GPT 系列"></a>GPT 系列</h2><p>Generative Pre-trained Transformer（GPT）系列是由 OpenAI 提出的一系列非常强大的预训练语言模型。</p>
<p>GPT模型的训练需要超大的训练语料，超多的模型参数以及超强的计算资源。GPT系列的模型结构秉承了不断堆叠transformer的思想，通过不断的提升训练语料的规模和质量，提升网络的参数数量来完成GPT系列的迭代更新的。GPT也证明了，通过不断的提升模型容量和语料规模，模型的能力是可以不断提升的。</p>
<p><img src="image-20230110090210989.png" alt="GPT系列"></p>
<h4 id="GPT-1"><a href="#GPT-1" class="headerlink" title="GPT-1"></a>GPT-1</h4><h5 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h5><p>在 GPT-1 之前，传统的 NLP 模型往往使用大量的数据对有监督的模型进行任务相关的模型训练，而有监督学习的任务存在两个缺点：</p>
<ol>
<li>需要大量的标注数据，高质量的标注数据往往很难获得，因为在很多任务中，图像的标签并不是唯一的或者实例标签并不存在明确的边界；</li>
<li>根据一个任务训练的模型很难泛化到其它任务中，这个模型只能叫做“领域专家”而不是真正的理解了NLP。</li>
</ol>
<h5 id="思想-1"><a href="#思想-1" class="headerlink" title="思想"></a>思想</h5><p>先通过在无标签的数据上学习一个生成式的语言模型，然后再根据特定的任务进行微调，即无监督的预训练+有监督的模型微调。</p>
<h5 id="结构-1"><a href="#结构-1" class="headerlink" title="结构"></a>结构</h5><p><img src="image-20230110101334539.png" alt="GPT-1"></p>
<ul>
<li>分类任务：将起始和终止token加入到原始序列两端，输入transformer中得到特征向量，最后经过一个全连接得到预测的概率分布；</li>
<li>自然语言推理：将前提（premise）和假设（hypothesis）通过分隔符（Delimiter）隔开，两端加上起始和终止token。再依次通过transformer和全连接得到预测结果；</li>
<li>语义相似度：输入的两个句子，正向和反向各拼接一次，然后分别输入给transformer，得到的特征向量拼接后再送给全连接得到预测结果；</li>
<li>问答和常识推理：将 n 个选项的问题抽象化为 n 个二分类问题，即每个选项分别和内容进行拼接，然后各送入transformer和全连接中，最后选择置信度最高的作为预测结果。</li>
</ul>
<h5 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h5><p>GPT-1使用了 BooksCorpus 数据集，这个数据集包含 7,000 本没有发布的书籍。作者选这个数据集的原因有二：1. 数据集拥有更长的上下文依赖关系，使得模型能学得更长期的依赖关系；2. 这些书籍因为没有发布，所以很难在下游数据集上见到，更能验证模型的泛化能力。</p>
<h4 id="GPT-2"><a href="#GPT-2" class="headerlink" title="GPT-2"></a>GPT-2</h4><p>GPT-2的目标旨在训练一个泛化能力更强的词向量模型，它并没有对GPT-1的网络进行过多的结构的创新与设计，只是使用了更多的网络参数和更大的数据集。</p>
<h5 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h5><p>作者认为，当一个语言模型的容量足够大时，它就足以覆盖所有的有监督任务，也就是说，任何有监督任务都是语言模型的一个子集，当模型的容量非常大且数据量足够丰富时，仅仅靠训练语言模型的学习便可以完成其他有监督学习的任务。</p>
<h5 id="数据集-1"><a href="#数据集-1" class="headerlink" title="数据集"></a>数据集</h5><p>GPT-2 的文章取自于 Reddit 上高赞的文章，命名为 WebText。数据集共有约800万篇文章，累计体积约 40G。为了避免和测试集的冲突，WebText 移除了涉及 Wikipedia 的文章。</p>
<h5 id="贡献-1"><a href="#贡献-1" class="headerlink" title="贡献"></a>贡献</h5><p>验证了通过海量数据和大量参数训练出来的词向量模型有迁移到其它类别任务中而不需要额外的训练的能力。</p>
<h4 id="GPT-3"><a href="#GPT-3" class="headerlink" title="GPT-3"></a>GPT-3</h4><p>强大：仅仅需要zero-shot或者few-shot，就可以在下游任务表现的非常好。</p>
<p>巨巨巨烧钱：1750亿的参数，31个分工明确的作者，超强算力的计算机（ 285,000 个CPU， 10,000 个 GPU），1200万的训练费用，45TB 的训练数据（维基百科的全部数据只相当于其中的 0.6% ）。</p>
<h5 id="meta-learning（元学习）"><a href="#meta-learning（元学习）" class="headerlink" title="meta-learning（元学习）"></a>meta-learning（元学习）</h5><p>对于一个少样本的任务来说，模型的初始化值非常重要，从一个好的初始化值作为起点，模型能够更快收敛。</p>
<p>元学习的核心思想在于通过少量的数据寻找一个合适的初始化范围，使得模型能够在有限的数据集上快速拟合，并获得不错的效果。</p>
<h5 id="In-context-learning（情境学习）"><a href="#In-context-learning（情境学习）" class="headerlink" title="In-context learning（情境学习）"></a>In-context learning（情境学习）</h5><p><img src="image-20230110121446447.png" alt="In-context learning"></p>
<h5 id="Without-fine-tuning"><a href="#Without-fine-tuning" class="headerlink" title="Without fine-tuning"></a>Without fine-tuning</h5><p>从理论上讲GPT-3也是支持fine-tuning的，但是fine-tuning需要利用海量的标注数据进行训练才能获得比较好的效果，但是这样也会造成对其它未训练过的任务上表现差，所以GPT-3并没有尝试fine-tuning。</p>
<h5 id="模型容量"><a href="#模型容量" class="headerlink" title="模型容量"></a>模型容量</h5><p>实验结果表明，三个学习方式的效果都会随着模型容量的上升而上升，且few shot &gt; one shot &gt; zero shot。</p>
<p><img src="image-20230110125804638.png" alt="容量upup，准确率upup"></p>
<h5 id="数据集-2"><a href="#数据集-2" class="headerlink" title="数据集"></a>数据集</h5><p>GPT-3共训练了5个不同的语料，分别是低质量的Common Crawl，高质量的 WebText2，Books1，Books2 和 Wikipedia，GPT-3根据数据集的不同的质量赋予了不同的权值，权值越高的在训练的时候越容易抽样到。</p>
<p><img src="image-20230110130508351.png" alt="不同数据集对应的结果"></p>
<h5 id="结构-2"><a href="#结构-2" class="headerlink" title="结构"></a>结构</h5><p>GPT-3沿用了GPT-2的结构，但是在网络容量上做了很大的提升，如采用了96层的多头 transformer。</p>
<h5 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h5><ol>
<li>对于一些命题没有意义的问题，GPT-3不会判断命题有效与否，而是拟合一个没有意义的答案出来；</li>
<li>由于 40TB 海量数据的存在，很难保证GPT-3生成的文章不包含一些非常敏感的内容，例如种族歧视，性别歧视，宗教偏见等；</li>
<li>受限于transformer的建模能力，GPT-3并不能保证生成的一篇长文章或者一本书籍的连贯性，存在下文不停重复上文的问题。</li>
</ol>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>GPT 系列从1到3，通通采用的是 transformer 架构，可以说模型结构并没有创新性的设计。</p>
<p>本质是通过海量的参数学习海量的数据，然后依赖transformer强大的拟合能力使得模型能够收敛。</p>
<h2 id="野生-GPT-系列"><a href="#野生-GPT-系列" class="headerlink" title="野生 GPT 系列"></a>野生 GPT 系列</h2><p>为了打破 OpenAI 和微软对自然语言处理 AI 模型的垄断，EleutherAI 研究并开源了一系列可以与类似规模的 GPT-3 模型相媲美的自然语言处理 AI 模型。</p>
<h4 id="GPT-Neo"><a href="#GPT-Neo" class="headerlink" title="GPT-Neo"></a>GPT-Neo</h4><p>EleutherAI 在 2021 年 3 月发布了 27 亿参数的 GPT-Neo 模型，这是他们对类 GPT 系统的第一个实现。GPT-Neo 是在 TensorFlow 中构建的，并通过 Mesh TensorFlow 并行库在 TPU 上训练。</p>
<p>模型参数级别：125 M，350 M，1.3 B，2.7 B。</p>
<p>GPT-Neo 开源模型里较大的版本也只达到了 GPT-3 商用版里最小模型的参数量。</p>
<p>训练效率低 + GPU 资源变多 → GPT-NeoX。</p>
<h4 id="GPT-J"><a href="#GPT-J" class="headerlink" title="GPT-J"></a>GPT-J</h4><p>GPT-J 是一个基于 GPT-3、由 60 亿个参数组成的自然语言处理 AI 模型。该模型在一个 800 GB 的开源文本数据集上进行训练，能够与类似规模的 GPT-3 模型相媲美。</p>
<p>GPT-J 使用新库 Mesh-Transformer-JAX 来训练，该库没有使用像 TensorFlow 这样的特定深度学习框架，而是使用 Google 的 JAX 线性代数框架。</p>
<p>与 GPT-Neo 模型相比，GPT-J 的训练效率提高了 125%。在几个 Down-Streaming 工作负载的零点性能方面，GPT-J 是公开的 Transformer LM 中表现最好的。</p>
<h4 id="GPT-NeoX"><a href="#GPT-NeoX" class="headerlink" title="GPT-NeoX"></a>GPT-NeoX</h4><h5 id="The-Pile"><a href="#The-Pile" class="headerlink" title="The Pile"></a>The Pile</h5><p>这是一个 825 GB 的用于训练的多样化文本数据集。The Pile 由 22 个不同的高质量子集构成，包括现有的和新建的，其中许多来源于学术领域或各专业领域。</p>
<h5 id="硬件"><a href="#硬件" class="headerlink" title="硬件"></a>硬件</h5><p>此前，在 GPT-Neo 和 GPT-J 的训练过程中，EleutherAI 都是通过 TPU Research Cloud (TRC) 访问抢占式 TPU，但想在合理的时间内用 TRC TPU 训练超过数百亿参数的模型是不现实的。</p>
<p>2021 年 1 月，EleutherAI 宣布与 CoreWeave 达成合作，CoreWeave 承诺为 GPT-NeoX-20B 模型训练提供 GPU 资源。研究者透露，他们在 96 个 A100 上完成了 GPT-NeoX-20B 的训练，训练成本约 86 万美元。</p>
<h5 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h5><p>GPT-NeoX-20B 总体优于 GPT-J-6B 等，但和 DaVinci 相比还存在一定差距。</p>
<p><img src="image-20230110145614826.png" alt="GPT-NeoX的表现"></p>
<h4 id="minGPT"><a href="#minGPT" class="headerlink" title="minGPT"></a>minGPT</h4><p><img src="image-20230110150224995.png" alt="如果将GPT比作军舰，那么minGPT就像一艘快艇"></p>
<p>使用 PyTorch 对 GPT 的重新实现，包括训练和推断。minGPT 试图变得小、干净、可解释和有教育意义，因为目前大多数可用的GPT模型实现可能有点杂乱。minGPT 只有大约300行代码。所要做的就是将一个索引序列输入到一个Transformer中，然后该序列中下一个索引的概率分布出来。作者指出，GPT 中大多数的复杂性只是为了提高效率而巧妙地处理批处理。</p>
<h2 id="自然语言-编程语言预训练模型（NL-PL）"><a href="#自然语言-编程语言预训练模型（NL-PL）" class="headerlink" title="自然语言-编程语言预训练模型（NL-PL）"></a>自然语言-编程语言预训练模型（NL-PL）</h2><h4 id="CodeBERT"><a href="#CodeBERT" class="headerlink" title="CodeBERT"></a>CodeBERT</h4><p>将BERT应用到了Python、PHP、Java、JavaScript、Go、Ruby等编程语言的代码搜索和生成任务当中。</p>
<h5 id="自然语言搜索代码"><a href="#自然语言搜索代码" class="headerlink" title="自然语言搜索代码"></a>自然语言搜索代码</h5><p>通过自然语言query查找到所需的代码块的实现和搜索引擎（通过自然语言query来查找所需网页）类似。</p>
<p>作者在 CodeSearchNet 语料库上对CodeBERT进行了预训练并做微调，这是一个包含了 6 种较为普遍的代码语言（分别为Ruby、JavaScript、Go、Python、Java、PHP）的语料库。最终，在自然语言代码搜索任务中取得了 SOTA 的结果。</p>
<p><img src="image-20230110171057473.png" alt="自然语言搜索代码"></p>
<h5 id="代码生成"><a href="#代码生成" class="headerlink" title="代码生成"></a>代码生成</h5><p>CodeBERT抓住了自然语言和编程语言之间的语义联系。</p>
<p><img src="image-20230110171336102.png" alt="自然语言生成代码"></p>
<h5 id="结构-3"><a href="#结构-3" class="headerlink" title="结构"></a>结构</h5><p>在模型的整体架构上，CodeBERT 并未脱离 BERT 和 Roberta 的思想。和大多数工作类似，作者使用了多层双向 Transformer。</p>
<p>在预训练阶段，总共设计了两部分输入，一个是自然语言文本，另一个是编程语言的代码。在模型训练的设计上，其主要包括两个目标：</p>
<ul>
<li>掩码语言建模（MLM）。将NL-PL对作为输入，随机为NL和PL选择位置进行掩码，然后用特殊的掩码Token进行替换。注意，掩码语言建模的任务是预测出被掩码的原始Token。</li>
<li>可替换Token检测（Replaced Token Detection (RTD)）。在这部分有两个数据生成器，分别是NL生成器和PL生成器，这两个生成器都用于随机掩码位置集（randomly masked positions）生成合理的备选方案。另外，还有一个学习生成器用来检测一个词是否为原词，其背后原理是一个二进制分类器。</li>
</ul>
<p><img src="image-20230110171745068.png" alt="模型架构"></p>
<p>模型训练的最后一步是模型微调，具体操作是在NL-PL任务中使用不同的CodeBERT设置。例如在自然语言代码搜索中，会使用与预训练阶段相同的输入方式。而在代码到文本的生成中，使用编码器-解码器框架，并使用CodeBERT初始化生成模型的编码器。</p>
<h5 id="泛化能力"><a href="#泛化能力" class="headerlink" title="泛化能力"></a>泛化能力</h5><p>在C#语言上的测试结果：</p>
<p><img src="image-20230110173002897.png" alt="用C#测试泛化能力"></p>
<p>从这个结果可以看出，相较于 RoBERTa，CodeBERT 能够更好地推广到其他编程语言。不过值得注意的是，模型的效果略低于code2seq，作者认为原因可能是 code2seq 在其抽象语法树中使用组合路径，而CodeBERT仅将原始代码作为输入。</p>
<h5 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h5><ol>
<li>首个大型的 NL-PL 预训练模型，且在自然语言代码搜索和代码文档生成两个任务中都达到了SOTA性能；</li>
<li>提出了一个混合学习目标，能够支持使用双模数据NL-PL，且能够很容易地应用到单模数据中（例如没有自然语言文本的编程代码）；</li>
<li>建立了一个用来研究 NL-PL 预训练模型的探测能力的数据集，方便了以后跟进的研究人员。</li>
</ol>
<h4 id="PyMT5"><a href="#PyMT5" class="headerlink" title="PyMT5"></a>PyMT5</h4><p>一个既可以从自然语言文档字符串（文档字符串）预测整个方法，又可以将代码总结为任何通用风格的文档字符串的单一模型。</p>
<h5 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h5><p>对2600万个 Python 方法和770万个 methoddocstring 对的大规模并行语料库进行了分析和建模，证明对于文档字符串和方法的生成，PyMT5优于类似大小的自动回归语言模型(GPT-2)。</p>
<h5 id="数据集-3"><a href="#数据集-3" class="headerlink" title="数据集"></a>数据集</h5><p>由 118k GITHUB 存储库组成，约 27 GB。</p>
<p>输入序列中的前导注释指示模型输出特征的特定组合，例如。“#target signature and body”指示 PyMT5 要预测签名和正文。</p>
<p><img src="image-20230110181309905.png" alt="PyMT5"></p>
<h5 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h5><p><img src="image-20230110182104513.png" alt="空格 → G"></p>
<p>Python 文件的空格替换为字符 G 进行标记，然后将标记的随机子序列替换为编号的掩码，并训练模型返回每个掩码后面跟着它所替换的标记。</p>
<h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><h4 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h4><p>官方代码和预训练模型：<a target="_blank" rel="noopener" href="https://github.com/google-research/bert">https://github.com/google-research/bert</a></p>
<p>Google官方推荐的PyTorch BERB版本实现，可加载Google预训练的模型：<a target="_blank" rel="noopener" href="https://github.com/huggingface/pytorch-pretrained-BERT">https://github.com/huggingface/pytorch-pretrained-BERT</a></p>
<h4 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h4><p>官方代码和预训练模型：<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/fairseq">https://github.com/facebookresearch/fairseq</a></p>
<p>中文预训练RoBERTa模型：<a target="_blank" rel="noopener" href="https://github.com/brightmart/roberta_zh">https://github.com/brightmart/roberta_zh</a></p>
<h4 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h4><p>GPT-1：<a target="_blank" rel="noopener" href="https://github.com/huggingface/pytorch-openai-transformer-lm">https://github.com/huggingface/pytorch-openai-transformer-lm</a></p>
<p>GPT-2：<a target="_blank" rel="noopener" href="https://github.com/openai/gpt-2">https://github.com/openai/gpt-2</a></p>
<p>GPT-3 API：<a target="_blank" rel="noopener" href="https://openai.com/blog/api-no-waitlist/">https://openai.com/blog/api-no-waitlist/</a> ，需要海外手机号注册</p>
<p>GPT-Neo：<a target="_blank" rel="noopener" href="https://github.com/EleutherAI/gpt-neo/">https://github.com/EleutherAI/gpt-neo/</a></p>
<p>GPT-J：<a target="_blank" rel="noopener" href="https://github.com/kingoflolz/mesh-transformer-jax">https://github.com/kingoflolz/mesh-transformer-jax</a></p>
<p>GPT-NeoX：<a target="_blank" rel="noopener" href="https://github.com/EleutherAI/gpt-neox/">https://github.com/EleutherAI/gpt-neox/</a></p>
<p>GPT-NeoX 试用：<a target="_blank" rel="noopener" href="https://goose.ai/">https://goose.ai/</a></p>
<p>minGPT：<a target="_blank" rel="noopener" href="https://github.com/karpathy/minGPT">https://github.com/karpathy/minGPT</a></p>
<h4 id="NL-PL"><a href="#NL-PL" class="headerlink" title="NL-PL"></a>NL-PL</h4><p>CodeBERT：<a target="_blank" rel="noopener" href="https://github.com/microsoft/CodeBERT">https://github.com/microsoft/CodeBERT</a></p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">ztrura</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="http://ztrura.com/2023/01/10/LanguageModels/">http://ztrura.com/2023/01/10/LanguageModels/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">ztrura</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/NLP/">
                                    <span class="chip bg-color">NLP</span>
                                </a>
                            
                                <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">
                                    <span class="chip bg-color">大模型学习笔记</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2023/01/13/LanguageModels2/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/10.jpg" class="responsive-img" alt="Language Models 补充">
                        
                        <span class="card-title">Language Models 补充</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            对上次语言模型综述的补充。
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2023-01-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/IDEAS-Lab/" class="post-category">
                                    IDEAS Lab
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/NLP/">
                        <span class="chip bg-color">NLP</span>
                    </a>
                    
                    <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">
                        <span class="chip bg-color">大模型学习笔记</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2023/01/01/2022blog/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/16.jpg" class="responsive-img" alt="2022blog">
                        
                        <span class="card-title">2022blog</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            跨年记。
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2023-01-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/personal/" class="post-category">
                                    personal
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E6%9D%82%E8%B0%88/">
                        <span class="chip bg-color">杂谈</span>
                    </a>
                    
                    <a href="/tags/%E8%B7%A8%E5%B9%B4/">
                        <span class="chip bg-color">跨年</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE' || selection.getRangeAt(0).commonAncestorContainer.nodeName === 'CODE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: Ztrura<br />'
            + '文章作者: Ztrura<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>


  <!-- 是否加载使用自带的 prismjs. -->
  <script type="text/javascript" src="/libs/prism/prism.min.js"></script>


<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="tencent"
                   type="playlist"
                   id="8752014121"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2019-2023</span>
            
            <a href="/about" target="_blank">Ztrura</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Ztrura" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:809772167@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=809772167" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 809772167" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>





    <a href="https://www.zhihu.com/people/waan-4" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/waan-4" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
